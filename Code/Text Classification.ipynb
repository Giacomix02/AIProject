#%% md
# Importiamo le librerie necessarie
#%%
# data manipulation
import pandas as pd

# numpy arrays
import numpy as np

# data visualization
import seaborn as sns

import matplotlib.pyplot as plt

import plotly
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.figure_factory as ff

sns.set()

# NLP
import string

from wordcloud import WordCloud

import nltk
from nltk.probability import FreqDist
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords

from Code.NLTKVectorizer import NLTKVectorizer

import re

# machine learning
from sklearn.datasets import fetch_20newsgroups

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline

from sklearn.linear_model import LogisticRegression  # Logistic Regression
from sklearn.naive_bayes import MultinomialNB  # Naive Bayes
from sklearn.svm import LinearSVC  # SVM
from sklearn.ensemble import RandomForestClassifier  # Random Forest

from sklearn.decomposition import TruncatedSVD

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

from sklearn.calibration import CalibratedClassifierCV

# Model explainability
from lime.lime_text import LimeTextExplainer

# other
from pprint import pprint
from time import time
import logging
from functools import partial
import joblib

#nltk
import nltk

nltk.download('punkt')

nltk.download('stopwords')

nltk.download('wordnet')

en_stop_words = list(set(stopwords.words("english")))

# aggiungo a en_stop_words le parole dal file da noi creato
with open('stopwordsPersonali', 'r') as file:
    for line in file:
        en_stop_words.append(line.strip())

#%%
# Setup dati iniziali
#%% md
 assegniamo i dataset di training e test alle variabili di train e test
#%%
dfTrain = pd.read_csv('Dataset/archive/train.csv')
dfTest = pd.read_csv('Dataset/archive/test.csv')
#%%
X_train = dfTrain['question_content']
y_train = dfTrain['topic']
X_test = dfTest['question_content']
y_test = dfTest['topic']
#%% md
## Generatore della color palette
#%% md
Questa funzione viene utilizata per creare una *palette* di `n` colori di `palette_name` colori.
#%%
def get_n_color_palette(palette_name, n_colors, as_hex=False):
    palette = sns.color_palette(palette=palette_name, n_colors=n_colors)
    if as_hex:
        palette = palette.as_hex()
    palette.reverse()
    return palette
#%% md
## Plotly export chart 
#%% md
Questa funzione è utilizzata per esportare l' HTML di plotly `fig_obj`, e salvarlo in: `assets/file_name.html`
#%%
def save_fig_as_div(fig_obj, file_name):
    with open(f"{file_name}", "w") as fig_file:
        fig_div_string = plotly.offline.plot(
            figure_or_data=fig_obj, output_type="div", include_plotlyjs="cdn"
        )
        fig_file.write(fig_div_string)
#%% md
## Report di classificazione
#%% md
La funzione genera il report di classificazione per le previsioni del modello
#%%
def get_classification_report(y_true, y_pred, target_names):

    # calcola il report di classificazione e lo converte in un DataFrame
    
    clf_report = classification_report(
        y_true=y_true, y_pred=y_pred, target_names=target_names, output_dict=True
    )
    clf_report_df = pd.DataFrame(data=clf_report)
    clf_report_df = clf_report_df.T
    clf_report_df.drop(columns=["support"], inplace=True)

    measures = clf_report_df.columns.tolist()
    classes = clf_report_df.index.tolist()

    # crea un heatmap annotato di plotly e aggiorna lo stile
    
    fig = ff.create_annotated_heatmap(clf_report_df.values, x=measures, y=classes)
    fig.update_layout(
        autosize=False,
        width=800,
        height=800,
        title_text="<i><b>Classification report</b></i>",
        xaxis_title="Measures",
        yaxis_title="Class",
        plot_bgcolor="rgba(0, 0, 0, 0)",
        paper_bgcolor="rgba(0, 0, 0, 0)",
        font={
            "family": "Courier New, monospace",
            "size": 14,
            # 'color': "#eaeaea"
        },
    )
    fig.update_xaxes(tickangle=-45)
    fig["data"][0]["showscale"] = True

    return fig
#%% md
## Funzione per il confusion matrix
#%% md
Questa funzione genera una confusion map per le previsioni del modello
#%%
def get_confusion_matrix(y_true, y_pred, labels):

    # calcola la confusion matrix
    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=labels)
    conf_matrix = np.flipud(conf_matrix)

    # crea una heatmap annotata della matrice di confusione
    fig = ff.create_annotated_heatmap(
        conf_matrix, x=labels.tolist(), y=labels.tolist()[::-1]
    )
    fig.update_layout(
        autosize=False,
        width=800,
        height=800,
        title_text="<i><b>Confusion matrix</b></i>",
        xaxis_title="Predicted category",
        yaxis_title="Real category",
        plot_bgcolor="rgba(0, 0, 0, 0)",
        paper_bgcolor="rgba(0, 0, 0, 0)",
        font={
            "family": "Courier New, monospace",
            "size": 14,
            # 'color': "#eaeaea"
        },
    )
    fig.update_xaxes(tickangle=-45)
    fig["data"][0]["showscale"] = True

    return fig
#%% md
# Data Statistics
#%% md
Calcola per ogni topic nel dataset il numero dei topic
#%%
categories_statistics_df = (
    dfTrain.groupby(by="topic")["id"]
    .agg(
        [
            ("count", lambda x: x.size),
        ]
    )
    .reset_index()
    .sort_values(by="count", ascending=False)
)
#%% md
Calcola per ogni topic nel dataset la lunghezza media delle domande per topic
#%%
categories_statistics_df_questions = (
    dfTrain.groupby(by="topic")["question_content"]
    .agg(
        [
            ("mean", lambda x: x.str.len().mean()),
            ("max", lambda x: x.str.len().max()),
            ("min", lambda x: x.str.len().min()),
        ]
    )
    .reset_index()
)
#%% md
## Categories article count:
#%% md
Usa un grafico a torta per mostrare le percentuali di articoli per ogni topic:
#%%
blue_palette = get_n_color_palette("Blues", 20, True)

fig = px.pie(
    data_frame=categories_statistics_df,
    names="topic",
    values="count",
    color_discrete_sequence=blue_palette,
    title="Percentuale di domande per topic",
    width=800,
    height=500,
)

fig.update_layout(
    {
        "plot_bgcolor": "rgba(0, 0, 0, 0)",
        "paper_bgcolor": "rgba(0, 0, 0, 0)",
        "font": {
            "family": "Courier New, monospace",
            "size": 14,
            # 'color': "#eaeaea"
        },
    }
)

fig.show()
#%% md

Possiamo vedere che il dataset è *bilanciato*
#%%
# salvo il grafico in un file html
save_fig_as_div(fig, file_name='charts/categories-percentages-pie-chart.html')
#%% md
## Lunghezza media delle domande per topic:
#%% md
Usa un diagramma a barre per mostrare la lunghezza media delle domande per ogni topic:
#%%
chart_labels = {"mean": "Lunghezza delle domande", "Topic": "Topic type"}

fig = px.bar(
    data_frame=categories_statistics_df_questions.sort_values(by="mean"),
    x="topic",
    y="mean",
    color="mean",
    labels=chart_labels,
    title="Lunghezza media delle domande per topic",
    width=800,
    height=500,
)

fig.update_layout(
    {
        "plot_bgcolor": "rgba(0, 0, 0, 0)",
        "paper_bgcolor": "rgba(0, 0, 0, 0)",
        "font": {
            "family": "Courier New, monospace",
            "size": 14,
            # 'color': "#eaeaea"
        },
    }
)

# rotate x-axis ticks
fig.update_xaxes(tickangle=-45)

fig.show()
#%% md
Notiamo che la lunghezza delle domande è ben distrubuita per tutti i topic, tranne per il topic 0 (Society & Culture), 8 (Family & Relationships) e 9 (Politics & Government), che hanno una lunghezza media delle domande leggermente più lunga rispetto agli altri topic.
#%%
save_fig_as_div(fig, file_name="charts/average-article-length-bar-chart.html")
#%% md
# Word Cloud
#%%
categories_text_df = dfTrain.groupby(by="topic").agg({"question_content": " ".join}).reset_index()
#%%
def plot_word_cloud(category_name, category_text):
    plt.subplots(figsize=(8, 8))
    wc = WordCloud(
        background_color="white", stopwords=en_stop_words, width=1000, height=600
    )
    wc.generate(category_text)
    plt.title(label=category_name)
    plt.axis("off")
    plt.imshow(wc, interpolation="bilinear")
    plt.show()
#%% md
La seguente word-cloud ci aiuterà a dare uno sguardo ai dati e al suo contenuto.

Per ogni word cloud, le parole con una frequenza maggiore hanno una dimensione maggiore.

Questo ci aiuterà a capire quali sono le parole più frequenti in ogni topic.
#%%
for idx, row in categories_text_df.iterrows():
    plot_word_cloud(row["topic"], row["question_content"])
#%% md
Notiamo che le parole più comunemente usate, che però non rispecchiano il topic sono: 
- think
- would
- get
- want

Quindi sono state aggiunte alla lista di stopwords.
#%% md
# Text Vectorization
#%% md
In questo passaggio, costruiremo un **text vectorization** transformer che verrà utilizzato per convertire le domande grezze in funzionalità appropriate, preparate per essere inserite negli algoritmi di machine learning.
#%% md
Definiamo un *custom vectorizer* chiamato `NLTKVectorizer`. Questo vectorizer eredita il `TfidfVectorizer` e sovrascrive il metodo `build_analyzer`. Il vectorizer risultante avrà gli stessi parametri del `TfidfVectorizer` ma analizzerà i documenti in modo diverso, principalmente utilizzerà il tokenzier `NLTK`, lemmatizer.
#%% md
## Vettoirzer e tipi di modelli
#%% md
Qui di seguito viene definito il vectorizer e i modelli che verranno presi in considerazione
#%%
# text vectorizer
vectorizer = NLTKVectorizer(
    stop_words=en_stop_words, max_df=0.5, min_df=10, max_features=10000
)

# Logistic Regression classifier
lr_clf = LogisticRegression(C=1.0, solver="newton-cg", multi_class="multinomial", n_jobs=2, verbose=2)

# Naive Bayes classifier
nb_clf = MultinomialNB(alpha=0.01)

# SVM classifier
svm_clf = LinearSVC(C=1.0, verbose=2)

# Random Forest classifier
random_forest_clf = RandomForestClassifier(
    n_estimators=100, criterion="gini", max_depth=50, random_state=0, n_jobs=2, verbose=2)


#%%
pipeline = Pipeline([("vect", vectorizer), ("clf", lr_clf)])
#%%

from sklearn.metrics import mean_squared_error

models = [lr_clf, nb_clf, svm_clf, random_forest_clf]
dict = {}
for model in models:
    pipeline = Pipeline([("vect", vectorizer), ("clf", model)])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    print(pipeline)
    dict[model] = mean_squared_error(y_test, y_pred)
print(sorted(dict.items(), key=lambda x: x[1]))

#%% md
## Risultati:
[(LinearSVC(verbose=2), 6.253166666666667), (LogisticRegression(multi_class='multinomial', n_jobs=2, solver='newton-cg',
                   verbose=2), 6.3049), (MultinomialNB(alpha=0.01), 6.743266666666667), (RandomForestClassifier(max_depth=50, n_jobs=2, random_state=0, verbose=2), 10.1255)]
                
Il modello migliore da utilizzare è il LinearSVC
#%% md
# Creazione della pipeline
#%% md
Creo la pipeline con il modello migliore e il vectorizer a cui passo le stopwords
#%%
# text vectorizer
vectorizer = NLTKVectorizer(stop_words=en_stop_words,
                            max_df=0.5, min_df=10, max_features=10000)

# logistic Regression classifier
lr_clf = LogisticRegression(C=1.0, solver='newton-cg', multi_class='multinomial', n_jobs=-1, verbose=2)

# create pipeline object
pipeline = Pipeline([
    ('vect', vectorizer),
    ('clf', lr_clf)
])

#%%
%%time
model = pipeline.fit(X_train, y_train)
#%%
%%time
y_pred = pipeline.predict(X_test)
#%% md
# Model Evaluation
#%% md
## Classification report:
#%%
fig = get_classification_report(
    y_true=y_test, y_pred=y_pred, target_names=pipeline.classes_
)
fig.show()
#%%
save_fig_as_div(fig_obj=fig, file_name="charts/classification-report.html")
#%% md
## Confusion matrix:
#%%
fig = get_confusion_matrix(y_true=y_test, y_pred=y_pred, labels=pipeline.classes_)
fig.show()
#%%
save_fig_as_div(fig_obj=fig, file_name="charts/confusion-matrix.html")
#%% md
# Model Explainability
#%% md
## Visualizing classifier weights:
#%%
feature_names = pipeline["vect"].get_feature_names()
#%%
clf = pipeline.named_steps["clf"]
#%%
coefs_values = sum(
    [classifier.base_estimator.coef_ for classifier in clf.calibrated_classifiers_]
)

coefs_values = coefs_values / len(clf.calibrated_classifiers_)
#%%
classes = clf.classes_
#%%
n_feature = 15
#%%
for i, class_label in enumerate(classes):

    # get indices of top positive/negative coefficient
    negative_coefs_indices = np.argsort(coefs_values[i])[:n_feature]
    positive_coefs_indices = np.argsort(coefs_values[i])[-n_feature:]

    # get the coefficient values
    negative_coefs = [coefs_values[i][coef_idx] for coef_idx in negative_coefs_indices]
    positive_coefs = [coefs_values[i][coef_idx] for coef_idx in positive_coefs_indices]

    # get the corresponding features names of the top coefficient
    negative_features = [feature_names[coef_idx] for coef_idx in negative_coefs_indices]
    positive_features = [feature_names[coef_idx] for coef_idx in positive_coefs_indices]

    # stack arrays into one array
    weights = np.concatenate([negative_coefs, positive_coefs])
    features = np.concatenate([negative_features, positive_features])

    # plot feature names agains their weight, using bar plot
    chart_title = f"{class_label} articles"

    chart_labels = {"x": "Feature name", "y": "Feature weight"}

    fig = px.bar(
        x=features,
        y=weights,
        color=weights,
        title=chart_title,
        orientation="v",
        labels=chart_labels,
        width=800,
        height=500,
        color_continuous_scale=["red", "blue"],
    )

    # rotate x-axis ticks
    fig.update_xaxes(tickangle=45)

    fig.update_layout(
        {
            "plot_bgcolor": "rgba(0, 0, 0, 0)",
            "paper_bgcolor": "rgba(0, 0, 0, 0)",
            "font": {
                "family": "Courier New, monospace",
                "size": 14,
                # 'color': "#eaeaea"
            },
        }
    )

    fig.show()


#%%
    save_fig_as_div(
        fig, file_name=f"model-coefficients/{class_label}-class-bar-chart.html"
    )
#%% md
## Explaining individual predictions:
#%%
clf_predictions_df = pd.DataFrame(
    data={
        "text": X_test.values,
        "real category": y_test.values,
        "predicted category": y_pred,
    }
)

clf_predictions_df["text length"] = clf_predictions_df["text"].str.len()

clf_predictions_df.sort_values(by="text length", ascending=False, inplace=True)
#%%
clf_predictions_df.head()
#%%
# samples which has been correctly classified
correct_classified_df = clf_predictions_df[
    clf_predictions_df["real category"] == clf_predictions_df["predicted category"]
]
#%%
# samples which has been incorrectly classified
incorrect_classified_df = clf_predictions_df[
    clf_predictions_df["real category"] != clf_predictions_df["predicted category"]
]
#%%
# preprocessing function used in the pipeline to analyze input documents
tokenize_fn = pipeline.named_steps["vect"].build_analyzer()
#%%
explainer = LimeTextExplainer(verbose=True, class_names=pipeline.classes_)
#%%
clf_fn = pipeline.predict_proba
#%% md
### When the model is performing well:
#%%
correct_sample = correct_classified_df.loc[[2573]]
#%%
correct_sample
#%%
text_to_explain = correct_sample["text"].values[0]
cleaned_text_to_explain = " ".join(tokenize_fn(text_to_explain))
#%%
exp_object = explainer.explain_instance(
    text_instance=cleaned_text_to_explain,
    classifier_fn=clf_fn,
    top_labels=2,
    num_features=10,
    num_samples=10000,
)
#%%
labels = exp_object.available_labels()
#%%
exp_object.show_in_notebook(labels=labels)
#%%
exp_object.save_to_file(
    file_path="assets/model-explanations/correct-classification-explanation.html",
    labels=labels,
)
#%% md
Here the classifier predicted the correct class of the sample
#%%
# politics_mideast class
exp_object.as_list(label=10)
#%% md
We can see that words like `omar` and `hand` are negative fearues for this class, and if we remove them from the document, the the prediction percentage of class `politics_mideast` would increase.
#%%
cleaned_text_to_explain = re.sub("omar|hand", "", cleaned_text_to_explain)
#%%
exp_object = explainer.explain_instance(
    text_instance=cleaned_text_to_explain,
    classifier_fn=clf_fn,
    top_labels=2,
    num_features=10,
    num_samples=10000,
)
#%%
labels = exp_object.available_labels()
#%%
exp_object.show_in_notebook(labels=labels)
#%% md
### When the model is performing badly:
#%%
incorrect_sample = incorrect_classified_df.loc[[3145]]
#%%
incorrect_sample
#%%
text_to_explain = incorrect_sample["text"].values[0]
cleaned_text_to_explain = " ".join(tokenize_fn(text_to_explain))
#%%
exp_object = explainer.explain_instance(
    text_instance=cleaned_text_to_explain,
    classifier_fn=clf_fn,
    top_labels=2,
    num_features=10,
    num_samples=10000,
)
#%%
labels = exp_object.available_labels()
#%%
exp_object.show_in_notebook(labels=labels)
#%% md
Here the classifier predicted an incorrect class, it predicted the class `autos` while the correct class is `forsale`, the reason is that the document very short, it contains two words about selling, and two words about cars, and the classification probabilities for the two classes were almost equal, let's try another example:
#%%
incorrect_sample = incorrect_classified_df.loc[[4251]]
#%%
incorrect_sample
#%% md
Here, the classifier had done the opposite of the previous case, it predicted the class `forsale` while the real class is `autos`.
#%%
text_to_explain = incorrect_sample["text"].values[0]
cleaned_text_to_explain = " ".join(tokenize_fn(text_to_explain))
#%%
exp_object = explainer.explain_instance(
    text_instance=cleaned_text_to_explain,
    classifier_fn=clf_fn,
    top_labels=2,
    num_features=10,
    num_samples=10000,
)
#%%
labels = exp_object.available_labels()
#%%
exp_object.show_in_notebook(labels=labels)
#%% md
What happened here is that the classifier had correlated words like `seat` and `owner` with class `autos`, and words like `sale`, `original` and `sell` with class `forsale`, we can inspect the sample more to understand why it's so confusing:
#%%
print(text_to_explain)
#%% md
Reading through the article, I *myself* actually got confused! so the article goes about someone criticizing the idea of posting car-seats ads, and he mentions two car models, the [Toyota MR2](https://en.wikipedia.org/wiki/Toyota_MR2) and the [Toyota Celica](https://en.wikipedia.org/wiki/Toyota_Celica), these two words are very informative to recognize the `autos` class, but it's hard for the classifier to catch them (since it's based on frequency statistics).
#%%
exp_object.save_to_file(
    file_path="assets/model-explanations/incorrect-classification-explanation.html",
    labels=labels,
)
#%% md
# Improving accuracy
#%% md
## Edit normalization
#%% md
After inspecting how the model is predicting classes, we saw that word `MR2` is actually relevant to the classification, but it's being removed by the analyzer, I'll change the value of `min_token_length` to 2, this way only tokens of two letters or less will be removed.
#%%
vectorizer = NLTKVectorizer(
    max_df=0.5,
    min_df=10,
    ngram_range=(1, 1),
    max_features=10000,
    stop_words=en_stop_words,
    min_token_length=2,
)

svm_clf = LinearSVC(C=1.0)
#%%
clf = CalibratedClassifierCV(base_estimator=svm_clf, cv=5, method="isotonic")
#%%
pipeline = Pipeline([("vect", vectorizer), ("clf", clf)])
#%%
%%time
pipeline.fit(X_train, y_train)
#%%
%%time
y_pred = pipeline.predict(X_test)
#%% md
## Classification report
#%%
fig = get_classification_report(
    y_true=y_test, y_pred=y_pred, target_names=pipeline.classes_
)
fig.show()
#%%
save_fig_as_div(fig_obj=fig, file_name="charts/classification-report-improved.html")
#%% md
## Confusion matrix
#%%
fig = get_confusion_matrix(y_true=y_test, y_pred=y_pred, labels=pipeline.classes_)
fig.show()
#%%
save_fig_as_div(fig_obj=fig, file_name="charts/confusion-matrix-improved.html")
#%% md
## AI model Dump
#%% md
Creiamo il dump del modello
#%%
from joblib import dump, load
#%%
dump(pipeline, 'dump/model.joblib')
#%% md
## Carichiamo il modello e utilizziamolo per fare delle previsioni
#%%
pipeline = load('dump/model.joblib')
#%%
text = input('Enter your question: ')

predictProba = pipeline.predict_proba([text])

array = ["Society & culture", "Science & Mathematics", "Health", "Education & Reference", "Computers & Internet", "Sports", "Business & Finance", "Entertainment & Music", "Family & Relationships", "Politics & Government"]
stat = predictProba[0][pipeline.predict([text])[0]] * 100

print("\"" + text + "\"" + "\n" " is about: " + "\"" + array[pipeline.predict([text])[0]]+ "\"" +"\n"+" with a probability of " +"%.3f" %stat + "%" "\n")

for i in range(10):
    print("\t\t\t"+ array[i] + " : " + "%.3f" % (predictProba[0][i] * 100) + "%")
#%%
